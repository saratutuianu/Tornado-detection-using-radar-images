{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saratutuianu/Tornado-detection-using-radar-images/blob/main/tornado_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uUGxjPZr6nM5"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./drive/MyDrive/proiect_ml/requirements/torch.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0b0tMziB5_b",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/proiect_ml')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import importlib, linecache\n",
        "\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "import tornet.data.preprocess as preprocess\n",
        "\n",
        "linecache.checkcache(preprocess.__file__)      # invalidează cache-ul de linii\n",
        "preprocess = importlib.reload(preprocess)\n",
        "\n",
        "from tornet.data.loader import read_file, TornadoDataLoader\n",
        "from tornet.data.preprocess import add_coordinates, permute_dims, remove_tilt_dim, get_shape\n",
        "from tornet.data.constants import ALL_VARIABLES\n",
        "\n",
        "data_root=r'./drive/MyDrive/proiect_ml/tornet_dataset'\n",
        "\n",
        "year = 2019\n",
        "\n",
        "catalog_path = os.path.join(data_root,'catalog.csv')\n",
        "\n",
        "# interpreteaza ca date si ore\n",
        "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
        "\n",
        "catalog['J'] = catalog['start_time'].dt.dayofyear\n",
        "catalog['r'] = catalog['J'] % 20\n",
        "catalog = catalog[catalog.start_time.dt.year == 2019]\n",
        "\n",
        "catalog_test = catalog[catalog['type'] == 'test']\n",
        "catalog_test = catalog_test.sample(frac=1,random_state=1234) # shuffles list\n",
        "\n",
        "catalog = catalog[catalog['type']=='train']\n",
        "\n",
        "catalog_train = catalog[catalog['r'] <= 13]\n",
        "catalog_train = catalog_train.sample(frac=1,random_state=1234) # shuffles list\n",
        "\n",
        "catalog_validation = catalog[(catalog['r'] > 13) & (catalog['r'] < 17)]\n",
        "catalog_validation = catalog_validation.sample(frac=1,random_state=1234) # shuffles list\n",
        "\n",
        "confirmed_number = len(catalog_train[catalog_train['category'] == 'TOR'])\n",
        "total_number = len(catalog_train)\n",
        "\n",
        "class TornadoDataset(TornadoDataLoader,Dataset):\n",
        "    pass\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            lambda d: remove_tilt_dim(d)\n",
        "            ])\n",
        "\n",
        "file_list_train = [os.path.join(data_root,f) for f in catalog_train.filename]\n",
        "file_list_validation = [os.path.join(data_root,f) for f in catalog_validation.filename]\n",
        "file_list_test = [os.path.join(data_root,f) for f in catalog_test.filename]\n",
        "\n",
        "torch_ds_train = TornadoDataset(file_list_train,\n",
        "                          variables=ALL_VARIABLES,\n",
        "                          n_frames=4,\n",
        "                          tilt_last=False, # so ordering of dims is [time,tilt,az,range]\n",
        "                          transform=transform)\n",
        "\n",
        "torch_ds_validation = TornadoDataset(file_list_validation,\n",
        "                          variables=ALL_VARIABLES,\n",
        "                          n_frames=4,\n",
        "                          tilt_last=False, # so ordering of dims is [time,tilt,az,range]\n",
        "                          transform=transform)\n",
        "\n",
        "torch_ds_test = TornadoDataset(file_list_test,\n",
        "                          variables=ALL_VARIABLES,\n",
        "                          n_frames=4,\n",
        "                          tilt_last=False, # so ordering of dims is [time,tilt,az,range]\n",
        "                          transform=transform)\n",
        "\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "torch_dl_train = torch.utils.data.DataLoader( torch_ds_train,\n",
        "                                        batch_size=batch_size,\n",
        "                                        num_workers=8 )\n",
        "\n",
        "torch_dl_validation = torch.utils.data.DataLoader( torch_ds_validation,\n",
        "                                        batch_size=batch_size,\n",
        "                                        num_workers=8 )\n",
        "\n",
        "torch_dl_test = torch.utils.data.DataLoader( torch_ds_test,\n",
        "                                        batch_size=batch_size,\n",
        "                                        num_workers=8 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTh_wMXVO3ld"
      },
      "outputs": [],
      "source": [
        "from tornet.models.torch.cnn_baseline import NormalizeVariable\n",
        "from tornet.data.constants import CHANNEL_MIN_MAX\n",
        "\n",
        "def conv3d_bn_block(in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "        nn.BatchNorm3d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def conv3d_transpose_bn_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "    )\n",
        "\n",
        "class TornadoLikelihood(nn.Module):\n",
        "    def __init__(self,radar_variables=ALL_VARIABLES):\n",
        "      super(TornadoLikelihood, self).__init__()\n",
        "      self.radar_variables=radar_variables\n",
        "\n",
        "      # Partea de encoder\n",
        "      self.conv_layer_encoder1 = conv3d_bn_block(in_channels=len(radar_variables), out_channels=16)\n",
        "      self.conv_layer_encoder2 = conv3d_bn_block(in_channels=16, out_channels=16)\n",
        "\n",
        "      self.max_pool_encoder1 = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
        "\n",
        "      self.conv_layer_encoder3 = conv3d_bn_block(in_channels=16, out_channels=32)\n",
        "      self.conv_layer_encoder4 = conv3d_bn_block(in_channels=32, out_channels=32)\n",
        "\n",
        "      self.max_pool_encoder2 = nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2))\n",
        "\n",
        "      self.conv_layer_encoder5 = conv3d_bn_block(in_channels=32, out_channels=64)\n",
        "      self.conv_layer_encoder6 = conv3d_bn_block(in_channels=64, out_channels=64)\n",
        "\n",
        "      self.max_pool_encoder3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "      self.conv_layer_encoder7 = conv3d_bn_block(in_channels=64, out_channels=128)\n",
        "      self.conv_layer_encoder8 = conv3d_bn_block(in_channels=128, out_channels=128)\n",
        "\n",
        "      self.max_pool_encoder4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "      # Partea de decoder\n",
        "      self.conv_layer_decoder1 = conv3d_bn_block(in_channels=128, out_channels=128)\n",
        "      self.conv_layer_decoder2 = conv3d_bn_block(in_channels=128, out_channels=128)\n",
        "\n",
        "      self.upsample_decoder1 = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "\n",
        "      self.conv_layer_decoder3 = conv3d_bn_block(in_channels=128, out_channels=64)\n",
        "      self.conv_layer_decoder4 = conv3d_bn_block(in_channels=64, out_channels=64)\n",
        "\n",
        "      self.upsample_decoder2 = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
        "\n",
        "      self.conv_layer_decoder5 = conv3d_bn_block(in_channels=64, out_channels=32)\n",
        "      self.conv_layer_decoder6 = conv3d_bn_block(in_channels=32, out_channels=32)\n",
        "\n",
        "      self.upsample_decoder3 = nn.Upsample(scale_factor=(1,2,2), mode='trilinear', align_corners=True)\n",
        "\n",
        "      self.conv_layer_decoder7 = conv3d_bn_block(in_channels=32, out_channels=16)\n",
        "      self.conv_layer_decoder8 = conv3d_bn_block(in_channels=16, out_channels=16)\n",
        "\n",
        "      self.upsample_decoder4 = nn.Upsample(scale_factor=(1,2,2), mode='trilinear', align_corners=True)\n",
        "\n",
        "      self.conv_layer_final = conv3d_transpose_bn_block(in_channels=16, out_channels=1)\n",
        "\n",
        "    def _normalize_inputs(self,data):\n",
        "        normed_data = {}\n",
        "        for v in self.radar_variables:\n",
        "            min_max = np.array(CHANNEL_MIN_MAX[v]) # [2,]\n",
        "            scale = 1/(min_max[1]-min_max[0])\n",
        "            offset = min_max[0]\n",
        "            normed_data[v] = (data[v] - offset) * scale\n",
        "\n",
        "        return normed_data\n",
        "\n",
        "    def forward(self,x):\n",
        "      \"\"\"\n",
        "      Assumes x contains radar varialbes on [batch,tilt,az,rng]\n",
        "      \"\"\"\n",
        "      # extract radar inputs\n",
        "      x = {v:x[v] for v in self.radar_variables} # each [batch,time,Az,Rng]\n",
        "      # normalize\n",
        "      x = self._normalize_inputs(x) # each [batch,time,Az,Rng]\n",
        "      # concatenate along channel (time) dim\n",
        "      x = torch.stack([x[v] for v in self.radar_variables], dim=1)\n",
        "\n",
        "      x = torch.where(torch.isnan(x),-3,x)\n",
        "\n",
        "      x = self.conv_layer_encoder1(x)\n",
        "      x = self.conv_layer_encoder2(x)\n",
        "      x = self.max_pool_encoder1(x)\n",
        "\n",
        "      x = self.conv_layer_encoder3(x)\n",
        "      x = self.conv_layer_encoder4(x)\n",
        "      x = self.max_pool_encoder2(x)\n",
        "\n",
        "      x = self.conv_layer_encoder5(x)\n",
        "      x = self.conv_layer_encoder6(x)\n",
        "      x = self.max_pool_encoder3(x)\n",
        "\n",
        "      x = self.conv_layer_encoder7(x)\n",
        "      x = self.conv_layer_encoder8(x)\n",
        "      x = self.max_pool_encoder4(x)\n",
        "\n",
        "      x = self.conv_layer_decoder1(x)\n",
        "      x = self.conv_layer_decoder2(x)\n",
        "      x = self.upsample_decoder1(x)\n",
        "\n",
        "      x = self.conv_layer_decoder3(x)\n",
        "      x = self.conv_layer_decoder4(x)\n",
        "      x = self.upsample_decoder2(x)\n",
        "\n",
        "      x = self.conv_layer_decoder5(x)\n",
        "      x = self.conv_layer_decoder6(x)\n",
        "      x = self.upsample_decoder3(x)\n",
        "\n",
        "      x = self.conv_layer_decoder7(x)\n",
        "      x = self.conv_layer_decoder8(x)\n",
        "      x = self.upsample_decoder4(x)\n",
        "\n",
        "      x = self.conv_layer_final(x)\n",
        "\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwrKOwlw8Urj"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "from torchmetrics.functional.classification import binary_auroc, binary_average_precision\n",
        "\n",
        "def accuracy(logits, y, tres):\n",
        "  correct = 0\n",
        "  for i in range(len(logits)):\n",
        "    if logits[i] >= tres and y[i] == 1:\n",
        "      correct += 1\n",
        "    elif logits[i] < tres and y[i] == 0:\n",
        "      correct += 1\n",
        "\n",
        "  return correct / float(len(y)) * 100.0\n",
        "\n",
        "def recall(logits, y, tres):\n",
        "  correct = 0\n",
        "  false_negatives = 0\n",
        "  for i in range(len(logits)):\n",
        "    if logits[i] >= tres and y[i] == 1:\n",
        "      correct += 1\n",
        "    elif logits[i] < tres and y[i] == 1:\n",
        "      false_negatives += 1\n",
        "\n",
        "  return 0.0 if correct + false_negatives == 0 else correct / (correct + false_negatives) * 100.0\n",
        "\n",
        "def precision(logits, y, tres):\n",
        "  correct = 0\n",
        "  false_positives = 0\n",
        "  for i in range(len(logits)):\n",
        "    if logits[i] >= tres and y[i] == 1:\n",
        "      correct += 1\n",
        "    elif logits[i] >= tres and y[i] == 0:\n",
        "      false_positives += 1\n",
        "\n",
        "  return 0.0 if correct + false_positives == 0 else correct / (correct + false_positives) * 100.0\n",
        "\n",
        "def f1_score(logits, y, tres):\n",
        "  p = precision(logits, y, tres)\n",
        "  r = recall(logits, y, tres)\n",
        "  return 0.0 if p + r == 0 else 2 * (p * r) / (p + r)\n",
        "\n",
        "def apply_metrics(logits, y, avg_train_loss, avg_val_loss, best_treshold):\n",
        "  acc = accuracy(logits, y, best_treshold)\n",
        "  rec = recall(logits, y, best_treshold)\n",
        "  prec = precision(logits, y, best_treshold)\n",
        "  f1 = f1_score(logits, y, best_treshold)\n",
        "  auroc = binary_auroc(logits, y)\n",
        "  auprc = binary_average_precision(logits, y)\n",
        "\n",
        "  print(f\"Accuracy: , {acc:.4f}\")\n",
        "  print(f\"Recall: , {rec:.4f}\")\n",
        "  print(f\"Precision: , {prec:.4f}\")\n",
        "  print(f\"F1 Score: , {f1:.4f}\")\n",
        "  print(f\"AUROC:  {auroc.item():.4f}\")\n",
        "  print(f\"AUPRC:  {auprc.item():.4f}\")\n",
        "\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "      'recall': rec,\n",
        "      'precision': prec,\n",
        "      'f1_score': f1,\n",
        "      'auroc': auroc.item(),\n",
        "      'auprc': auprc.item(),\n",
        "      'avg_train_loss': avg_train_loss,\n",
        "      'avg_val_loss': avg_val_loss,\n",
        "      'threshold': best_treshold\n",
        "  }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recall mai important! -> e mai important sa flaguiasca toate tornadele, decat sa nu emita avertizari negative\n",
        "!pip install plotnine\n",
        "import plotnine\n",
        "from plotnine import *\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "def find_best_tres(logits, y, beta):\n",
        "  precision, recall, thresholds = precision_recall_curve(y, logits)\n",
        "\n",
        "  # Plot the precision-recall curve\n",
        "  df_recall_precision = pd.DataFrame({'Precision':precision[:-1],\n",
        "                                      'Recall':recall[:-1],\n",
        "                                      'Threshold':thresholds})\n",
        "  df_recall_precision.head()\n",
        "\n",
        "  plotnine.options.figure_size = (8, 4.8)\n",
        "\n",
        "  # Creat a data viz\n",
        "  plot1 = (\n",
        "      ggplot(data = df_recall_precision) +\n",
        "      geom_point(aes(x='Recall', y='Precision'), size=0.4) +\n",
        "      geom_line(aes(x='Recall', y='Precision')) +\n",
        "      labs(title='Recall Precision Curve') +\n",
        "      xlab('Recall') +\n",
        "      ylab('Precision') +\n",
        "      theme_minimal()\n",
        "  )\n",
        "  display(plot1)\n",
        "\n",
        "  precision = precision[:-1]\n",
        "  recall = recall[:-1]\n",
        "\n",
        "  den = (beta**2)*precision + recall\n",
        "  fbeta_score = np.where(den > 0, (1 + beta**2) * precision * recall / den, -np.inf)\n",
        "\n",
        "  # Find the optimal threshold\n",
        "  index = np.argmax(fbeta_score)\n",
        "  thresholdOpt = round(thresholds[index], ndigits = 4)\n",
        "  fscoreOpt = round(fbeta_score[index], ndigits = 4)\n",
        "  recallOpt = round(recall[index], ndigits = 4)\n",
        "  precisionOpt = round(precision[index], ndigits = 4)\n",
        "  print('Best Threshold: {} with F-Score: {}'.format(thresholdOpt, fscoreOpt))\n",
        "  print('Recall: {}, Precision: {}'.format(recallOpt, precisionOpt))\n",
        "\n",
        "  plotnine.options.figure_size = (8, 4.8)\n",
        "\n",
        "  # Create a data viz\n",
        "  plot2 = (\n",
        "    ggplot(data = df_recall_precision)+\n",
        "    geom_point(aes(x = 'Recall',\n",
        "                    y = 'Precision'),\n",
        "                size = 0.4)+\n",
        "    # Best threshold\n",
        "    geom_point(aes(x = recallOpt,\n",
        "                    y = precisionOpt),\n",
        "                color = '#981220',\n",
        "                size = 4)+\n",
        "    geom_line(aes(x = 'Recall',\n",
        "                  y = 'Precision'))+\n",
        "    # Annotate the text\n",
        "    geom_text(aes(x = recallOpt,\n",
        "                  y = precisionOpt),\n",
        "              label = 'Optimal threshold \\n for class: {}'.format(thresholdOpt),\n",
        "              nudge_x = 0.18,\n",
        "              nudge_y = 0,\n",
        "              size = 10,\n",
        "              fontstyle = 'italic')+\n",
        "    labs(title = 'Recall Precision Curve')+\n",
        "    xlab('Recall')+\n",
        "    ylab('Precision')+\n",
        "    theme_minimal()\n",
        "  )\n",
        "\n",
        "  display(plot2)\n",
        "\n",
        "  return thresholdOpt\n"
      ],
      "metadata": {
        "id": "HQrz833PWpQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import logging\n",
        "\n",
        "training_logs = []\n",
        "class TrainingLogger:\n",
        "    def __init__(self):\n",
        "        logging.basicConfig(level=logging.INFO, format='%(message)s', filename='/content/drive/MyDrive/proiect_ml/training.log', filemode='a')\n",
        "\n",
        "    def on_epoch_begin(self, epoch):\n",
        "        self.epoch_start_time = time()\n",
        "        logging.info(f\"Epoch {epoch + 1} starting.\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        elapsed_time = time() - self.epoch_start_time\n",
        "        logging.info(f\"Epoch {epoch + 1} finished in {elapsed_time:.2f} seconds.\")\n",
        "        logs['epoch_time'] = elapsed_time  # Add epoch time to logs\n",
        "        training_logs.append(logs)  # Collect training logs\n",
        "        logging.info(f\"Epoch {epoch + 1}: Val loss = {logs['avg_val_loss']:.4f}, Accuracy = {logs['accuracy']:.4f}, Recall = {logs['recall']:.4f}, Precision = {logs['precision']:.4f}, F1 = {logs['f1_score']:.4f}, AUROC = {logs['auroc']:.4f}, AUPRC = {logs['auprc']:.4f}\")\n"
      ],
      "metadata": {
        "id": "1ebH4pzJ9CjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIJRviy1KmYa"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "class RMSpropFromScratch(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, alpha=0.99, eps=1e-8,\n",
        "                 weight_decay=0.0, momentum=0.0, centered=False):\n",
        "\n",
        "        defaults = dict(lr=lr, alpha=alpha, eps=eps,\n",
        "                        weight_decay=weight_decay, momentum=momentum,\n",
        "                        centered=centered)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            alpha = group['alpha']\n",
        "            eps = group['eps']\n",
        "            wd = group['weight_decay']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "\n",
        "                # weight decay: g <- g + wd * w\n",
        "                if wd != 0.0:\n",
        "                    grad = grad.add(p, alpha=wd)\n",
        "\n",
        "                state = self.state[p]\n",
        "                # init state\n",
        "                if len(state) == 0:\n",
        "                    state[\"square_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)  # E[g^2]\n",
        "\n",
        "                    if group[\"momentum\"] > 0:\n",
        "                      state[\"momentum_buffer\"] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    if group[\"centered\"]:\n",
        "                      state[\"grad_avg\"] = torch.zeros_like(p, memory_format=torch.preserve_format)  # E[g]\n",
        "\n",
        "                square_avg = state['square_avg']\n",
        "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
        "\n",
        "                if group['centered']:\n",
        "                    grad_avg = state['grad_avg']\n",
        "                    grad_avg.mul_(alpha).add_(grad, alpha=1 - alpha)\n",
        "                    # var ≈ E[g^2] - (E[g])^2\n",
        "                    avg = square_avg.addcmul(grad_avg, grad_avg, value=-1).sqrt_()\n",
        "                else:\n",
        "                    avg = square_avg.sqrt()\n",
        "\n",
        "                avg = avg.add_(eps)\n",
        "\n",
        "                if group['momentum'] > 0.0:\n",
        "                    buf = state['momentum_buffer']\n",
        "                    buf.mul_(['momentum']).addcdiv_(grad, avg)\n",
        "                    p.add_(buf, alpha=-lr)\n",
        "                else:\n",
        "                    p.addcdiv_(grad, avg, value=-lr)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import logging\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from time import time\n",
        "\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "best_val = 10\n",
        "\n",
        "model = TornadoLikelihood()\n",
        "\n",
        "pos_weight = torch.tensor([(total_number - confirmed_number)/confirmed_number])  # weight asociat cu clasa pozitiva\n",
        "loss_f = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # daca w_p > 1 => favorizeaza recall, invers => precision\n",
        "\n",
        "optimizer = RMSpropFromScratch(model.parameters(), lr=1e-3)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# TensorBoard setup\n",
        "writer = SummaryWriter('/content/drive/MyDrive/proiect_ml/summary')\n",
        "\n",
        "ckpt_path = \"/content/drive/MyDrive/proiect_ml/checkpoints/best.pth\"\n",
        "ckpt = torch.load(ckpt_path)\n",
        "\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "\n",
        "start_epoch = int(ckpt[\"epoch\"]) + 1   # <- va fi 1\n",
        "best_val = float(ckpt.get(\"val_loss\", 1e9))\n",
        "\n",
        "print(f\"Reluare din {ckpt_path} | start_epoch={start_epoch} | best_val={best_val:.4f}\")\n",
        "\n",
        "best_treshold = 0.6\n",
        "\n",
        "avg_val_loss = best_val\n",
        "avg_train_loss = 0.0"
      ],
      "metadata": {
        "id": "1D03Yu7yulR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ii192qVskex"
      },
      "outputs": [],
      "source": [
        "\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "callback = TrainingLogger()\n",
        "\n",
        "# Training loop\n",
        "total_step = len(torch_dl_train)\n",
        "\n",
        "eps = 0.05\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    callback.on_epoch_begin(epoch)\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(torch_dl_train):\n",
        "        y = torch.squeeze(batch.pop('label'))\n",
        "        y_float = y[:, -1].float()\n",
        "\n",
        "        # De ce label smoothing? Nu e niciodata 100% ca e tornada\n",
        "        y_smooth = y_float * (1 - 2*eps) + eps\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(batch) # [batch,1,T,L,W]\n",
        "        logits = F.max_pool3d(logits, kernel_size=logits.size()[2:]) # [batch,1,1,1,1]\n",
        "        logits = torch.squeeze(logits) # [batch,1] for binary classifications\n",
        "        loss = loss_f(logits, y_smooth)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = train_loss / len(torch_dl_train)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\n",
        "    writer.add_scalar('training loss', avg_train_loss, epoch)\n",
        "\n",
        "    ckpt_path = f\"/content/drive/MyDrive/proiect_ml/checkpoints/epoch_{epoch+1:03d}.pth\"\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"val_loss\": avg_train_loss,\n",
        "    }, ckpt_path)\n",
        "\n",
        "    # Validation\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(torch_dl_validation):\n",
        "            y = torch.squeeze(batch.pop('label'))\n",
        "            y_float = y[:, -1].float()\n",
        "            y_int = y[:, -1].long()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(batch) # [batch,1,T,L,W]\n",
        "            logits = F.max_pool3d(logits, kernel_size=logits.size()[2:]) # [batch,1,1,1,1]\n",
        "            logits = torch.squeeze(logits) # [batch,1] for binary classifications\n",
        "            loss = loss_f(logits, y_float)\n",
        "\n",
        "            all_probs.append(torch.sigmoid(logits))\n",
        "            all_labels.append(y_int)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(torch_dl_validation)\n",
        "        writer.add_scalar('validation loss', avg_val_loss, epoch)\n",
        "        all_probs = torch.cat(all_probs, dim = 0)\n",
        "        all_labels = torch.cat(all_labels, dim = 0)\n",
        "\n",
        "    if avg_val_loss < best_val:\n",
        "        best_val = avg_val_loss\n",
        "        best_path = \"/content/drive/MyDrive/proiect_ml/checkpoints/best.pth\"\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"val_loss\": avg_val_loss,\n",
        "        }, best_path)\n",
        "        print(f\"[Epoch {epoch+1}] Best model salvat la {best_path} (val_loss={best_val:.4f})\")\n",
        "\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    logs = apply_metrics(all_probs, all_labels, avg_train_loss, avg_val_loss, best_treshold)\n",
        "\n",
        "    best_treshold = find_best_tres(all_probs, all_labels, 2)\n",
        "\n",
        "    callback.on_epoch_end(epoch, logs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test\n",
        "\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(torch_dl_test):\n",
        "        y = torch.squeeze(batch.pop('label'))\n",
        "        y_float = y[:, -1].float()\n",
        "        y_int = y[:, -1].long()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(batch) # [batch,1,T,L,W]\n",
        "        logits = F.max_pool3d(logits, kernel_size=logits.size()[2:]) # [batch,1,1,1,1]\n",
        "        logits = torch.squeeze(logits) # [batch,1] for binary classifications\n",
        "\n",
        "        all_probs.append(torch.sigmoid(logits))\n",
        "        all_labels.append(y_int)\n",
        "\n",
        "all_probs = torch.cat(all_probs, dim = 0)\n",
        "all_labels = torch.cat(all_labels, dim = 0)\n",
        "logs = apply_metrics(all_probs, all_labels, avg_train_loss, avg_val_loss, best_treshold)\n",
        "\n",
        "callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "Pl_hxRZkCtz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1TmDWD6ZoNrM7k3y6VjaTxove4X-58koB",
      "authorship_tag": "ABX9TyNeAgRayGpkumkeSWZCz0M5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}